* readme

** current models in the repository

1. Model0.py - a very simple model focused on stacks. It doesn't
   include the notion of a solution stack, just the stacks that exist
   in the world. The model has a world state, a recurse operator, the
   ability to check whether two objects fit well, and then the ability
   to stack two objects. All operations take a world state and produce
   a world state.
2. Model1.py - a stack model similar to Model0. The additional
   complexity here is that the agent must now choose between using a
   loose or a tight fit.
3. Model2.py - a stack model similar to Model0. This model is focused
   on capturing just the first stage we notice empirically - grabbing
   two objects and stacking them.

** useful models

What models, however, do we actually want? The answer depends on what
we hope to show with these models. If that's unclear, the choice of
models will be haphazard. We have several options, each of which
highlights something interesting about the learning process:

1. the development of procedural knowledge: the progression from a
   random one-off strategy to an iterative strategy to a
   quasi-divide-and-conquer strategy.
2. the development of an understanding of relevant features: the size
   of individual cups -- as opposed to their color, weight, or pattern
   -- is the relevant feature here. Furthermore, it's the tightness of
   fit between cups, their relative sizes, that is of crucial
   importance. The task can be solved entirely without reference to
   absolute size so long as relative size (tightness of fit) can be
   established. Monitoring the development of this knowledge is
   monitoring the development not of entirely new conceptual knowledge
   (most likely), but of the application of this knowledge to a
   specific problem.
3. the development of new representations: the creation stacks as a
   representational unit
4. type polymorphism: the development of knowledge about how stacks
   are similar or dissimilar to cups, and how objects can be viewed
   simultaneously as stacks and cups
5. error recovery: the development of algorithms that can recover even
   after some wrong action has been taken

A set of models covering all these aspects of learning would be
impressive. Let's take them in order. What models do we need to
demonstrate (1)? I think we should be able to use a stack-based
approach and simply show via the history of the MCMC chain how the
model progresses over time from simpler to more sophisticated
algorithms.

Then, to capture (2), we just need to increase the complexity of the
objects in the model. We can add features other than size that either
are (e.g. weight) or are not (e.g. color) correlated with size and see
how the model learns to adjust its strategies over the course of the
MCMC chain.

At that point, we'll have a fairly mature model of the basic process
and will be able to then look at (3).


Eventually, we probably want to modify the posterior to consider the
efficiency of computation as well as the height and complexity of the
program. We'll have to decide whether that's something that goes into
the prior or the likelihood, though.
